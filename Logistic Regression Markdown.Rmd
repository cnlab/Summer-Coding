---
title: "Logisitic Regression"
output: html_notebook
---
Load in the appropriate packages 

```{r}
require('mlbench')
require('MASS')
require('pROC')
require('broom')
```

Load in the data, read it, and rename to a more code-friendly name. 
```{r}
data("PimaIndiansDiabetes2")

View(PimaIndiansDiabetes2)

###rename to more concise name 
df <- PimaIndiansDiabetes2
str(df)
View(df)
```
Check out the class of our variables and get our summary stats
```{r}
class(df$pregnant)
class(df$glucose)
class(df$pressure)
class(df$triceps)
class(df$insulin)
class(df$mass)
class(df$pedigree)
class(df$age)
class(df$diabetes)

#can also do this more concisely using the line 
str(df)
###get the descriptives of our variables 
summary(df)
```

We see a decent amount of missing data so we'll need to deal with that by running the next line. 
```{r}
newdf <- na.omit(df)
summary(newdf) ###can now see we got rid of all of them 
```

Now we need to check the distributions of our variables.
```{r}
##you'll need to increase the size of your plots window if you are running this in R studio and not an R markdown file
par(mfrow = c(4,2)) ##specifies we want to make a four row two column graphic
for( i in 1:8){
  hist(newdf[,i], main = colnames(newdf)[i],xlab = colnames(newdf)[i], col = 'blue')
}
```

From the  histograms, it is evident that Pregnant and Age are highly skewed, we'll need to create categories for them. You could transform the data but you need to have a strong theoretical reason to. Far too often we see people transforming data for unguided reasons without really understanding what they're doing. Sometimes it's ok if data look like that, but you need to take into account what you're studying.

This is where we begin to drop the different variables into "buckets" or "bins" to deal with non-normal data. 
```{r}
newdf$age_bucket <- as.factor(ifelse(newdf$age<=30,"20-30",ifelse(newdf$age<=40,"31-40",ifelse(newdf$age<=50,"41-50","50+"))))
###the as.factor creates a new variable "as a factor", ifelse is used to create the category only if it is true, in this case if it less than or equal to the specified category
newdf$pregnant_bucket <- as.factor(ifelse(newdf$pregnant<=5,"0–5",ifelse(newdf$pregnant<=10,"6–10","10+")))
###on the line above we are basing our buckets off of the categories in the histogram, if they are less than or equal to the specified
view(newdf)
```

Now going to check the distributions of our independent variables using boxplots 
```{r}
par(mfrow = c(3,2)) ###making a 3X2 graph
boxplot(glucose~diabetes, ylab="Glucose", xlab= "Diabetes", col="light blue",data = newdf)
boxplot(pressure~diabetes, ylab="Pressure", xlab= "Diabetes", col="light blue",data = newdf)
boxplot(triceps~diabetes, ylab="triceps", xlab= "Diabetes", col="light blue",data = newdf)
boxplot(insulin~diabetes, ylab="Insulin", xlab= "Diabetes", col="light blue",data = newdf)
boxplot(mass~diabetes, ylab="Mass", xlab= "Diabetes", col="light blue",data = newdf)
boxplot(pedigree~diabetes, ylab="Pedigree", xlab= "Diabetes", col="light blue",data = newdf)
##from these graphs we can infer certain things about people who have diabetes (e.g., they have elevated levels of glucose)
```

If you remember earlier we put our categorical IVs into buckets we can also use crosstabs to look at raw frequencies
```{r}
xtabs(~diabetes + age_bucket, data = newdf)
xtabs(~diabetes + pregnant_bucket, data = newdf)
```

Now we can create a new data frame with our variables of interest 
```{r}
newdf2 <- newdf[,c("diabetes","glucose","pressure","triceps","insulin","mass","pedigree","age_bucket","pregnant_bucket")] ###put names after comma to indicate you want the columns.
str(newdf2)
View(newdf2)
```

Now we can start building our logistic regression model 

```{r}
logit_1 <- glm(diabetes~., family = binomial,data = newdf2) ### this runs a logistic regression for all of our predictors in the dataset
summary(logit_1)

```
This is how we get our odds ratio and confidence intervals.
```{r}
###getting our odds ration 
(exp(coef(logit_1))) #obtaining odds ratio
##can also do this 
tidy(logit_1, exponentiate = TRUE, conf.level = 0.95) #odds ratio
###confidence intervals, which are important. if it crosses zero it isn't significant
confint(logit_1)
```


Building a smaller model that we can graph using ggpredict. 
```{r}
logit3 <- glm(diabetes~ glucose + insulin, data=newdf2,family = binomial)
summary(logit3)
tidy(logit3, exponentiate = TRUE, conf.level = 0.95) #odds ratio 
confint(logit3) #confidence intervals
```
Visualizing our model
```{r}
ggPredict(logit3,interactive=TRUE)
```

# Logistic Regression write up
A binary logistic regression was conducted to examine if there was a relationship between glucose and insulin levels on possessing a diabetes diagnosis. Glucose levels were found to significantly predict diabetes diagnosis, *B* = 0.042, *SE* = 0.005, *p* < .001,*OR* = 1.043,  *95% CI OR* [ 0.03, 0.05], with the odds of having diabetes increasing 4% for every one unit increase in age. However, we did not observe a significant relationship between insulin levels and diabetes, B* = 0.0001, *SE* = 0.001, *p* = 0.932,*OR* = 1.000,  *95% CI OR* [-0.002,  0.002].
